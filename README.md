# Federated Distillation of Natural Language Understanding with Confident Sinkhorns

This repository provides an alternative method for ensembled distillation of local models to a global model. The local models can be trained via entropy or optimal transport (OT) loss. We train local (on-device) models using cross-entropy loss due to higher computational complexity of OT. The global model is pretrained on global dataset which is relatively bigger than local datasets.

[Read the paper](https://arxiv.org/pdf/2110.02432.pdf)

README will be updated soon.
